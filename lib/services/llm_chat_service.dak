import 'dart:async';
import 'package:llama_flutter_android/llama_flutter_android.dart';
import 'llm_download_service.dart';


class LLMChatService {
  LlamaController? _llamaController;
  final LLMDownloadService _downloadService = LLMDownloadService();


  bool _isModelLoaded = false;
  bool get isModelLoaded => _isModelLoaded;


  // Optimized for mobile speed - smaller is faster
  static const int MAX_CONTEXT_TOKENS = 256;  // Further reduced for speed
  static const int MAX_RESPONSE_TOKENS = 150; // Shorter responses = faster
  static const int RESERVED_TOKENS = MAX_RESPONSE_TOKENS + 30; 
  static const int AVAILABLE_HISTORY_TOKENS = MAX_CONTEXT_TOKENS - RESERVED_TOKENS;


  // Minimal system prompt saves processing time
  static const String SYSTEM_PROMPT = """You are a supportive AI helping students. Be warm and concise.""";


  Future<bool> loadModel() async {
    try {
      final modelPath = await _downloadService.getModelPath();
      if (modelPath == null) {
        return false;
      }


      _llamaController ??= LlamaController();


      // Optimized loading parameters for speed
      await _llamaController!.loadModel(
        modelPath: modelPath,
        contextSize: MAX_CONTEXT_TOKENS,
        threads: 2,  // Reduced from 4 - better for mobile
      );


      _isModelLoaded = true;
      print('Model loaded with context: $MAX_CONTEXT_TOKENS');
      return true;
    } catch (e) {
      _isModelLoaded = false;
      return false;
    }
  }


  int _estimateTokens(String text) {
    return (text.length / 4).ceil();
  }


  // Keep only last 2 exchanges maximum
  List<Map<String, String>> _manageContextWindow(
    List<Map<String, String>> conversationHistory,
    String currentMessage,
  ) {
    final systemTokens = _estimateTokens(SYSTEM_PROMPT);
    final currentMessageTokens = _estimateTokens(currentMessage);
    
    int availableForHistory = AVAILABLE_HISTORY_TOKENS - systemTokens - currentMessageTokens;
    
    if (availableForHistory <= 0) {
      return [];
    }


    // Keep only last 4 messages (2 exchanges) for speed
    final recentHistory = conversationHistory.length > 4 
        ? conversationHistory.sublist(conversationHistory.length - 4)
        : conversationHistory;


    final List<Map<String, String>> fittingHistory = [];
    int usedTokens = 0;


    for (int i = recentHistory.length - 1; i >= 0; i--) {
      final message = recentHistory[i];
      final messageText = '${message['role']}: ${message['content']}';
      final messageTokens = _estimateTokens(messageText);


      if (usedTokens + messageTokens <= availableForHistory) {
        fittingHistory.insert(0, message);
        usedTokens += messageTokens;
      } else {
        break;
      }
    }


    return fittingHistory;
  }


  Stream<String> generateResponse(String userMessage, List<Map<String, String>> conversationHistory) async* {
    if (!_isModelLoaded || _llamaController == null) {
      yield 'Please download the AI model from settings first.';
      return;
    }


    try {
      final managedHistory = _manageContextWindow(conversationHistory, userMessage);
      final prompt = _buildPrompt(userMessage, managedHistory);
      
      print('Generating with ~${_estimateTokens(prompt)} tokens');


      final buffer = StringBuffer();
      int tokenCount = 0;
      bool hasYielded = false;


      await for (final token in _llamaController!.generate(
        prompt: prompt,
        maxTokens: MAX_RESPONSE_TOKENS,
        temperature: 0.8,      // Slightly higher for variety
        topK: 30,              // Reduced for faster sampling
        topP: 0.92,            // Adjusted for balance
        repeatPenalty: 1.05,
      )) {
        tokenCount++;
        hasYielded = true;
        
        // Check for stop patterns less frequently (every 5 tokens)
        if (tokenCount % 5 == 0 && _shouldStopGeneration(buffer.toString() + token)) {
          break;
        }
        
        buffer.write(token);
        yield token;
      }


      if (!hasYielded || buffer.isEmpty) {
        yield 'I had trouble responding. Try a shorter message.';
      }
    } catch (e) {
      print('Generation error: $e');
      yield 'Error occurred. Please try again with a shorter message.';
    }
  }


  bool _shouldStopGeneration(String currentText) {
    // Very simple stop check - only on double newlines
    return currentText.contains('\n\nUser:') || 
           currentText.contains('\n\nHuman:') ||
           currentText.length > 800; // Hard limit for safety
  }
 

  String _buildPrompt(String userMessage, List<Map<String, String>> history) {
    final buffer = StringBuffer();
    buffer.write(SYSTEM_PROMPT);
    buffer.write('\n\n');


    // Add only managed history
    for (final msg in history) {
      buffer.write('${msg['role'] == 'user' ? 'User' : 'Assistant'}: ${msg['content']}\n');
    }


    buffer.write('User: $userMessage\nAssistant:');
    return buffer.toString();
  }


  Future<String> generateSimpleResponse(String userMessage) async {
    if (!_isModelLoaded || _llamaController == null) {
      return 'Please download the AI model from settings.';
    }


    try {
      // Truncate very long messages
      final truncatedMessage = userMessage.length > 200 
          ? userMessage.substring(0, 200) 
          : userMessage;
          
      final prompt = '$SYSTEM_PROMPT\n\nUser: $truncatedMessage\nAssistant:';


      final buffer = StringBuffer();
      int tokenCount = 0;


      await for (final token in _llamaController!.generate(
        prompt: prompt,
        maxTokens: MAX_RESPONSE_TOKENS,
        temperature: 0.8,
        topK: 30,
        topP: 0.92,
        repeatPenalty: 1.05,
      )) {
        tokenCount++;
        if (tokenCount % 5 == 0 && _shouldStopGeneration(buffer.toString() + token)) {
          break;
        }
        buffer.write(token);
      }


      final response = buffer.toString().trim();
      return response.isNotEmpty ? response : 'Try a shorter message.';
    } catch (e) {
      print('Error: $e');
      return 'Error occurred. Please try a shorter message.';
    }
  }


  Map<String, dynamic> getContextStats(List<Map<String, String>> history, String currentMessage) {
    final managedHistory = _manageContextWindow(history, currentMessage);
    final prompt = _buildPrompt(currentMessage, managedHistory);
    final promptTokens = _estimateTokens(prompt);
    
    return {
      'totalMessages': history.length,
      'fittingMessages': managedHistory.length,
      'droppedMessages': history.length - managedHistory.length,
      'estimatedPromptTokens': promptTokens,
      'maxContextTokens': MAX_CONTEXT_TOKENS,
      'availableResponseTokens': MAX_RESPONSE_TOKENS,
      'contextUsagePercent': ((promptTokens / MAX_CONTEXT_TOKENS) * 100).toStringAsFixed(1),
    };
  }


  Future<void> stopGeneration() async {
    try {
      await _llamaController?.stop();
    } catch (e) {
      print('Stop error: $e');
    }
  }


  Future<void> unloadModel() async {
    try {
      await _llamaController?.dispose();
      _llamaController = null;
      _isModelLoaded = false;
    } catch (e) {
      print('Unload error: $e');
    }
  }


  Future<bool> isModelAvailable() async {
    final modelPath = await _downloadService.getModelPath();
    return modelPath != null;
  }


  void dispose() {
    unloadModel();
  }
}
